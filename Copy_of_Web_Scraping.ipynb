{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Web Scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUlTwSiqyAKf0iWiL+yhuL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdsimar1901/foss-web-scraping-tutorial/blob/main/Copy_of_Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xab5FUXmsgtp",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import requests # pip install requests\n",
        "from bs4 import BeautifulSoup  #pip install BeautifulSoup\n",
        "#https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ciPejAdssQU",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "url = \"https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\" # url of the website\n",
        "page = requests.get(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ornPfkA3uEv9",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "soup = BeautifulSoup(page.content) # This is creating an instance of the soup\n",
        "print(soup.prettify()) # Printing the html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn36BNCry1W8"
      },
      "source": [
        "Starting to use Beautiful Soup for Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3IU0DjqzAZv",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "first_header = soup.find('h2') # find the first element\n",
        "print(first_header)\n",
        "\n",
        "headers = soup.find_all('h2') #find all elements\n",
        "print(headers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyoaZfwIzMeo",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# we can also pass a list of elements\n",
        "\n",
        "elt = soup.find(['h1','h2'])\n",
        "print(elt)\n",
        "\n",
        "\n",
        "elts = soup.find_all(['h2','h1'])\n",
        "print(elts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSN39o0CznsC",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# we can also do an targeted search with the help of attributes\n",
        "\n",
        "elm = soup.find_all('div',attrs={\"id\":\"mw-page-base\"})\n",
        "print(elm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okpol5ww0ga2",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# We can also nest find/find_all combinations\n",
        "body = soup.find('body')\n",
        "div = body.find('footer')\n",
        "ul = div.find('ul')\n",
        "print(ul)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_pnYq5q1QCx",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# we can search for specific strings in the html\n",
        "contact = soup.find_all('li',string =re.compile('September'))\n",
        "print(contact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeMpLABh3KJL",
        "cellView": "form"
      },
      "source": [
        "#@title\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBPhSwX44ADo"
      },
      "source": [
        "Select with CSS Selectors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq6VhUjj4FOa",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#https://www.w3schools.com/cssref/css_selectors.asp\n",
        "\n",
        "\n",
        "content = soup.select('p a')\n",
        "content #select is similar to the find_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__xMibfF5oCA",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "soup.head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10p_evZt7GlG",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "soup.title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scs2Wqyu7mWg",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "stri = soup.find_all(string = ['Contact',\"World\"])\n",
        "stri"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgkFOQkc7o25",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# Getting different propertires of html\n",
        "#Smaller amount use .string\n",
        "\n",
        "#if multiple element use .get_text\n",
        "header = soup.find('head')\n",
        "strngg = header.get_text()\n",
        "strngg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qtk6vE5pBQzh",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# get a specific property from an element\n",
        "\n",
        "link = soup.find('a')\n",
        "link.href\n",
        "\n",
        "#use\n",
        "link['href']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUS0REzzBStm",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "soup.body.div.string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efo0KcM_EDhK",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# 3 terms\n",
        "#parent\n",
        "#sibling\n",
        "#child"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebfD2tbXEWHI",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "soup.body.find('footer').find_next_sibiling(s)\n",
        "\n",
        "# This is an error now but will lookup to it soon ....!!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNcrMbkiFXeZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azZ2BP73wsVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b785910b-00fb-4722-ab86-f65e072ed2a2"
      },
      "source": [
        "import urllib.request \n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL TO SCRAP\n",
        "wiki = \"https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\"\n",
        "#Query the website and return the html to the variable 'page'\n",
        "#For python 3 use urllib.request.urlopen(wiki)\n",
        "page = urllib.request.urlopen(wiki) \n",
        "\n",
        "#Parse the html in the 'page' variable, and store it in Beautiful Soup format\n",
        "soup = BeautifulSoup(page,features='html.parser')\n",
        "print('\\n\\nPage Scrapped !!!\\n\\n')\n",
        "\n",
        "\n",
        "print('\\n\\nTITLE OF THE PAGE\\n\\n')\n",
        "print(soup.title.string)\n",
        "\n",
        "print('\\n\\nALL THE URLs IN THE WEB PAGE\\n\\n')\n",
        "\n",
        "all_links = soup.find_all('a')\n",
        "\n",
        "print('Total number of URLs present = ',len(all_links)) \n",
        "\n",
        "print('\\n\\nLast 5 URLs in the page are : \\n')\n",
        "\n",
        "if len(all_links) > 5 :\n",
        "  \n",
        "  last_5 = all_links[len(all_links)-5:]\n",
        "  for url in last_5 :\n",
        "    print(url.get('href'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Page Scrapped !!!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "TITLE OF THE PAGE\n",
            "\n",
            "\n",
            "List of state and union territory capitals in India - Wikipedia\n",
            "\n",
            "\n",
            "ALL THE URLs IN THE WEB PAGE\n",
            "\n",
            "\n",
            "Total number of URLs present =  455\n",
            "\n",
            "\n",
            "Last 5 URLs in the page are : \n",
            "\n",
            "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
            "https://stats.wikimedia.org/#/en.wikipedia.org\n",
            "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
            "https://wikimediafoundation.org/\n",
            "https://www.mediawiki.org/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VSTh7aZwyNq"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from csv import writer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yuX23Cxw00A"
      },
      "source": [
        "url=\"https://www.worldometers.info/world-population/\"\n",
        "response= requests.get(url).text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsLiL5aqxLSx"
      },
      "source": [
        "soup = BeautifulSoup(response, \"html.parser\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9HC2BChxOh_"
      },
      "source": [
        "tabl = soup.find(\"table\", class_=\"table table-hover table-condensed\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-p_zFaxRyT"
      },
      "source": [
        "header = []  # Created Empty List\n",
        "# Extract header row\n",
        "for i in tabl.find_all('th'):\n",
        "    header.append(i.text)\n",
        "\n",
        "with open(\"world_population_by_region.csv\", \"wt\",newline='',encoding='utf-8') as csv_file:\n",
        "    csv_writer = writer(csv_file)\n",
        "    csv_writer.writerow(header) # write header\n",
        "    # Write data to csv file\n",
        "    for row in tabl.find_all('tr')[1:]:\n",
        "        td = row.find_all('td')\n",
        "        r = [i.text.replace('\\n','') for i in td]\n",
        "        csv_writer.writerow(r)"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}